{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuyendv/anaconda3/envs/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 14:24:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode:  train\n",
      "cuda:  cuda\n",
      "Random seed set as 8888\n",
      "use: xlm roberta\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tuyendv/projects/text-restoration-model/test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.202/home/tuyendv/projects/text-restoration-model/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m cuda \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.202/home/tuyendv/projects/text-restoration-model/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcuda: \u001b[39m\u001b[39m\"\u001b[39m, cuda)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.202/home/tuyendv/projects/text-restoration-model/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m trainer(cuda\u001b[39m=\u001b[39;49mcuda, mode\u001b[39m=\u001b[39;49mmode, is_warm_up\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, bert_type\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mbert)\n",
      "File \u001b[0;32m~/projects/text-restoration-model/src/trainer.py:25\u001b[0m, in \u001b[0;36mtrainer.__init__\u001b[0;34m(self, cuda, is_warm_up, mode, infer_path, bert_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m cuda\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_type \u001b[39m=\u001b[39m bert_type\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m bert_bilstm(\n\u001b[1;32m     26\u001b[0m     cuda\u001b[39m=\u001b[39;49mcuda,\n\u001b[1;32m     27\u001b[0m     nb_label\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mnb_labels, \n\u001b[1;32m     28\u001b[0m     bert_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_type)\u001b[39m.\u001b[39mto(cuda)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtokenizer\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), \n\u001b[1;32m     33\u001b[0m     lr\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mlr, \n\u001b[1;32m     34\u001b[0m     weight_decay\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mweight_decay)\n",
      "File \u001b[0;32m~/projects/text-restoration-model/src/model/bert_bilstm.py:27\u001b[0m, in \u001b[0;36mbert_bilstm.__init__\u001b[0;34m(self, nb_label, cuda, bert_type)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39muse: xlm roberta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m XLMRobertaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mxlm-roberta-base\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m XLMRobertaModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mxlm-roberta-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbilstm \u001b[39m=\u001b[39m BiLSTM(\n\u001b[1;32m     30\u001b[0m     cuda\u001b[39m=\u001b[39mcuda, \n\u001b[1;32m     31\u001b[0m     embedding_dim\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mhidden_dim_bert, \n\u001b[1;32m     32\u001b[0m     hidden_dim\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mhidden_dim_lstm\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(hparams\u001b[39m.\u001b[39mdrop_rate)\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/modeling_utils.py:1058\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1055\u001b[0m config\u001b[39m.\u001b[39mname_or_path \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Instantiate model.\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   1060\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m from_tf:\n\u001b[1;32m   1061\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:690\u001b[0m, in \u001b[0;36mRobertaModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m RobertaEmbeddings(config)\n\u001b[0;32m--> 690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m RobertaEncoder(config)\n\u001b[1;32m    692\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m RobertaPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:457\u001b[0m, in \u001b[0;36mRobertaEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([RobertaLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:457\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([RobertaLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:374\u001b[0m, in \u001b[0;36mRobertaLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mchunk_size_feed_forward\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m RobertaAttention(config)\n\u001b[1;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_cross_attention \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39madd_cross_attention\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:291\u001b[0m, in \u001b[0;36mRobertaAttention.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    290\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself \u001b[39m=\u001b[39m RobertaSelfAttention(config)\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m RobertaSelfOutput(config)\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpruned_heads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:161\u001b[0m, in \u001b[0;36mRobertaSelfAttention.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n\u001b[0;32m--> 161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_head_size)\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mattention_probs_dropout_prob)\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.9/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.resources import hparams\n",
    "from src.trainer import trainer\n",
    "from typing import Union\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = hparams.mode\n",
    "    print(\"mode: \", mode)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"cuda: \", cuda)\n",
    "        trainer = trainer(cuda=cuda, mode=mode, is_warm_up=True, bert_type=hparams.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trainer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<pad>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁rồi', '▁trong', '▁không', '▁gian', '▁r', 'ỗ', 'ng', '▁này', '▁những', '▁v', 'ầ', 'ng', '▁sáng', '▁trong', '▁không', '▁gian', '▁trở', '▁nên', '▁ab', 'c', 'de', 'fr', 'gh', 'd', 'fs', '▁hữu', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(data[8][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0,   1563,    238,    112,   7079,   9486,     71,  17447,   6348,\n",
      "          1000,    687,   6051,   1690,  66421,    449,   1617,   1358,     81,\n",
      "        249994,    449,  12107,   1000,    687,   6051,   9293,   3809,   1563,\n",
      "           238,    112,   7079,   9486,     71,  17447,  20527,   1563,    238,\n",
      "           112,   7079,   9486,     71,  17447,   6348,   1000,    687,   6051,\n",
      "          1690,  66421,    449,   1617,   1358,     81, 249994,    449,  12107,\n",
      "          1000,    687,   6051,   9293,   3809,   1563,    238,    112,   7079,\n",
      "          9486,     71,  17447,  20527,   1563,    238,    112,   7079,   9486,\n",
      "            71,  17447,   6348,   1000,    687,   6051,   1690,  66421,    449,\n",
      "          1617,   1358,     81, 249994,    449,  12107,   1000,    687,   6051,\n",
      "          9293,   3809,   1563,    238,    112,   7079,   9486,     71,  17447,\n",
      "         20527,   1563,    238,    112,   7079,   9486,     71,  17447,   6348,\n",
      "          1000,    687,   6051,   1690,  66421,    449,   1617,   1358,     81,\n",
      "        249994,    449,  12107,   1000,    687,   6051,   9293,   3809,   1563,\n",
      "           238,    112,   7079,   9486,     71,  17447,  20527,   1563,    238,\n",
      "           112,   7079,   9486,     71,  17447,   6348,   1000,    687,   6051,\n",
      "          1690,  66421,    449,   1617,   1358,     81, 249994,    449,  12107,\n",
      "          1000,    687,   6051,   9293,   3809,   1563,    238,    112,   7079,\n",
      "          9486,     71,  17447,  20527,   1563,    238,    112,   7079,   9486,\n",
      "            71,  17447,   6348,   1000,    687,   6051,   1690,  66421,    449,\n",
      "          1617,   1358,     81, 249994,    449,  12107,   1000,    687,   6051,\n",
      "          9293,   3809,   1563,    238,    112,   7079,   9486,     71,  17447,\n",
      "         20527,   1563,    238,    112,   7079,   9486,     71,  17447,   6348,\n",
      "          1000,    687,   6051,   1690,  66421,    449,   1617,   1358,     81,\n",
      "        249994,    449,  12107,   1000,    687,   6051,   9293,   3809,   1563,\n",
      "           238,    112,   7079,   9486,     71,  17447,  20527,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1])\n"
     ]
    }
   ],
   "source": [
    "print(data[8][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47b352d8bca17c9bd2193cdbe35d3eecbcd6a8825c23815536dcf4b48a4e466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
