{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envibert Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import RobertaModel\n",
    "from fairseq.models.roberta import XLMRModel\n",
    "from src.model.bilstm import BiLSTM\n",
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = \"xlmr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: xlm roberta\n"
     ]
    }
   ],
   "source": [
    "if bert_type ==\"envibert_cased\":\n",
    "    print(\"use: envibert\")\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "            os.path.join(hparams.toknizer_path,'envibert_tokenizer.py')).load_module().RobertaTokenizer(hparams.toknizer_path)\n",
    "    bert = RobertaModel.from_pretrained('nguyenvulebinh/envibert',cache_dir=hparams.pretrained_envibert_cased)\n",
    "elif bert_type ==\"envibert_uncased\":\n",
    "    print(\"use: envibert_uncased\")\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "            os.path.join(hparams.pretrained_envibert_uncased,'envibert_tokenizer.py')).load_module().RobertaTokenizer(hparams.pretrained_envibert_uncased)\n",
    "    bert = XLMRModel.from_pretrained(hparams.pretrained_envibert_uncased, checkpoint_file='model.pt')\n",
    "elif bert_type ==\"xlmr\":\n",
    "    print(\"use: xlm roberta\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    bert = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_bilstm(nn.Module):\n",
    "    def __init__(self, nb_label, cuda, bert, drop_rate, hidden_dim_lstm, hidden_dim_bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bilstm = BiLSTM(\n",
    "            cuda=cuda, \n",
    "            embedding_dim=hidden_dim_bert, \n",
    "            hidden_dim=hidden_dim_lstm\n",
    "            )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_lstm, nb_label).to(cuda)\n",
    "\n",
    "    def forward(self, input_ids, input_masks):\n",
    "        output = self.bert(input_ids=input_ids, \n",
    "                        attention_mask = input_masks)\n",
    "        sequence_output, _ = output[0], output[1]\n",
    "        \n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        sequence_output = self.bilstm(sequence_output)\n",
    "        output = self.linear(sequence_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_bilstm(\n",
    "        cuda=\"cuda:1\",\n",
    "        nb_label=hparams.nb_labels, \n",
    "        bert=bert,\n",
    "        drop_rate=hparams.drop_rate,\n",
    "        hidden_dim_bert=768,\n",
    "        hidden_dim_lstm=hparams.hidden_dim_lstm).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.load(\"/home/tuyendv/projects/text-restoration-model/checkpoint/xlmr/checkpoint_xlmr_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "for key, value in tmp.items():\n",
    "    res[key.replace(\"module.\",\"\")] = value\n",
    "model.load_state_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"checkpoint_xlmr_1_chatbot.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/home/tuyendv/projects/restore-text/checkpoint_xlmr_3.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1,2,3,4]])\n",
    "label_ids = torch.tensor([[1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tuyendv/projects/restore-text/remove_module.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.21.202/home/tuyendv/projects/restore-text/remove_module.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model(input_ids ,label_ids)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "model(input_ids ,label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envibert uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import RobertaModel\n",
    "from fairseq.models.roberta import XLMRModel\n",
    "from src.model.bilstm import BiLSTM\n",
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a47b352d8bca17c9bd2193cdbe35d3eecbcd6a8825c23815536dcf4b48a4e466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
