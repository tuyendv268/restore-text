{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envibert Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuyendv/anaconda3/envs/ner-capu/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-21 20:51:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import RobertaModel\n",
    "from fairseq.models.roberta import XLMRModel\n",
    "from src.model.bilstm import BiLSTM\n",
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = \"xlmr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: xlm roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if bert_type ==\"envibert_cased\":\n",
    "    print(\"use: envibert\")\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "            os.path.join(hparams.toknizer_path,'envibert_tokenizer.py')).load_module().RobertaTokenizer(hparams.toknizer_path)\n",
    "    bert = RobertaModel.from_pretrained('nguyenvulebinh/envibert',cache_dir=hparams.pretrained_envibert_cased)\n",
    "elif bert_type ==\"envibert_uncased\":\n",
    "    print(\"use: envibert_uncased\")\n",
    "    tokenizer = SourceFileLoader(\"envibert.tokenizer\", \n",
    "            os.path.join(hparams.pretrained_envibert_uncased,'envibert_tokenizer.py')).load_module().RobertaTokenizer(hparams.pretrained_envibert_uncased)\n",
    "    bert = XLMRModel.from_pretrained(hparams.pretrained_envibert_uncased, checkpoint_file='model.pt')\n",
    "elif bert_type ==\"xlmr\":\n",
    "    print(\"use: xlm roberta\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    bert = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_bilstm(nn.Module):\n",
    "    def __init__(self, nb_label, cuda, bert, drop_rate, hidden_dim_lstm, hidden_dim_bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bilstm = BiLSTM(\n",
    "            cuda=cuda, \n",
    "            embedding_dim=hidden_dim_bert, \n",
    "            hidden_dim=hidden_dim_lstm\n",
    "            )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.linear = nn.Linear(hidden_dim_lstm, nb_label).to(cuda)\n",
    "\n",
    "    def forward(self, input_ids, input_masks):\n",
    "        output = self.bert(input_ids=input_ids, \n",
    "                        attention_mask = input_masks)\n",
    "        sequence_output, _ = output[0], output[1]\n",
    "        \n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        sequence_output = self.bilstm(sequence_output)\n",
    "        output = self.linear(sequence_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_bilstm(\n",
    "        cuda=\"cuda:0\",\n",
    "        nb_label=hparams.nb_labels, \n",
    "        bert=bert,\n",
    "        drop_rate=hparams.drop_rate,\n",
    "        hidden_dim_bert=768,\n",
    "        hidden_dim_lstm=hparams.hidden_dim_lstm).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.load(\"/home/tuyendv/projects/restore-text/checkpoint/xlmr/checkpoint_xlmr_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "for key, value in tmp.items():\n",
    "    res[key.replace(\"module.\",\"\")] = value\n",
    "model.load_state_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"/home/tuyendv/projects/serving-model/restore-model-xlmr/checkpoint/restore_model_xlmr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"envibert_cased.pt\", map_location=\"cpu\").to(\"cpu\")\n",
    "model.cuda=\"cpu\"\n",
    "model.bilstm.cuda=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1,2,3,4]])\n",
    "label_ids = torch.tensor([[1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4678, -1.3652,  2.9402,  0.9296,  0.9075, -1.0552, -0.6884,\n",
       "          -0.7643, -1.6479],\n",
       "         [ 5.6365, -0.8979,  3.7384,  0.9441, -1.7830, -1.7870,  1.0364,\n",
       "          -1.9206, -0.6754],\n",
       "         [-3.3766, -2.8819, -3.0972,  2.8551, -3.8219,  0.1179,  2.5970,\n",
       "          -4.4640, -0.8132],\n",
       "         [-4.7732, -3.0532, -3.4493,  3.8218,  0.0276,  0.3232, -0.1796,\n",
       "          -3.0593, -3.1788]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids ,label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envibert uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import RobertaModel\n",
    "from fairseq.models.roberta import XLMRModel\n",
    "from src.model.bilstm import BiLSTM\n",
    "from src.resources import hparams\n",
    "from torch import nn\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ner-capu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f16b837acba259baf9f9713a5b16407f054afaae9f70bd47c46cba4cf7ecc7d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
